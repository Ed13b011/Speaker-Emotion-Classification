{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries \n",
    "\n",
    "# Keras\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Other  \n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob \n",
    "import os\n",
    "import pickle\n",
    "import IPython.display as ipd  # To play audio in notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation & processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>male_fear</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>male_disgust</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source                                               path\n",
       "0   male_neutral  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...\n",
       "1      male_fear  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...\n",
       "2  male_surprise  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...\n",
       "3       male_sad  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...\n",
       "4   male_disgust  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csv file created in extracting labels and path \n",
    "\n",
    "df = pd.read_csv(\"All_Data_Frames.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[-3.0082576, -5.508383, -11.619889, -11.603205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[-10.467623, -8.993829, -11.7763815, -13.74424...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[-12.981858, -8.290445, -9.102726, -10.725917,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[-23.589674, -24.579994, -22.594236, -21.48121...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[-22.956581, -23.019234, -20.55793, -19.553026...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-3.0082576, -5.508383, -11.619889, -11.603205...\n",
       "1  [-10.467623, -8.993829, -11.7763815, -13.74424...\n",
       "2  [-12.981858, -8.290445, -9.102726, -10.725917,...\n",
       "3  [-23.589674, -24.579994, -22.594236, -21.48121...\n",
       "4  [-22.956581, -23.019234, -20.55793, -19.553026..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MFCC is well known to be a good feature, MFCC-> Mel-frequency cepstral coefficient\n",
    "# read each audio file, extract its mean across all MFCC bands by time\n",
    "\n",
    "# feature data frame that is MFCC output for each file, and it's a 2D matrix of the number of bands by time\n",
    "feature_df = pd.DataFrame(columns=['feature'])\n",
    "\n",
    "# loop feature extraction over the entire dataset\n",
    "counter=0\n",
    "for index,path in enumerate(df.path):\n",
    "    X, sample_rate = librosa.load(path\n",
    "                                  , res_type='kaiser_fast'\n",
    "                                  ,duration=2.5\n",
    "                                  ,sr=44100\n",
    "                                  ,offset=0.5\n",
    "                                 )\n",
    "    sample_rate = np.array(sample_rate)\n",
    "    \n",
    "    # mean as the feature. Could do min and max etc as well. \n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                        sr=sample_rate, \n",
    "                                        n_mfcc=13), axis=0)\n",
    "    feature_df.loc[counter] = [mfccs]\n",
    "    counter=counter+1   \n",
    "\n",
    "# check feature data frame\n",
    "print(len(feature_df))\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-3.008258</td>\n",
       "      <td>-5.508383</td>\n",
       "      <td>-11.619889</td>\n",
       "      <td>-11.603205</td>\n",
       "      <td>-13.251027</td>\n",
       "      <td>-12.154624</td>\n",
       "      <td>-12.164827</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.975243</td>\n",
       "      <td>-27.885740</td>\n",
       "      <td>-27.087734</td>\n",
       "      <td>-29.094940</td>\n",
       "      <td>-26.330254</td>\n",
       "      <td>-25.438536</td>\n",
       "      <td>-25.739513</td>\n",
       "      <td>-26.417332</td>\n",
       "      <td>-25.851837</td>\n",
       "      <td>-25.928373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>male_fear</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-10.467623</td>\n",
       "      <td>-8.993829</td>\n",
       "      <td>-11.776381</td>\n",
       "      <td>-13.744241</td>\n",
       "      <td>-15.344474</td>\n",
       "      <td>-15.923333</td>\n",
       "      <td>-13.486449</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.035086</td>\n",
       "      <td>-7.168881</td>\n",
       "      <td>-6.481255</td>\n",
       "      <td>-6.759146</td>\n",
       "      <td>-6.833916</td>\n",
       "      <td>-6.347439</td>\n",
       "      <td>-5.493447</td>\n",
       "      <td>-5.551433</td>\n",
       "      <td>-0.891208</td>\n",
       "      <td>2.769547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-12.981858</td>\n",
       "      <td>-8.290445</td>\n",
       "      <td>-9.102726</td>\n",
       "      <td>-10.725917</td>\n",
       "      <td>-10.041571</td>\n",
       "      <td>-13.318968</td>\n",
       "      <td>-15.079294</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.084118</td>\n",
       "      <td>-26.953909</td>\n",
       "      <td>-28.918743</td>\n",
       "      <td>-28.394785</td>\n",
       "      <td>-26.758858</td>\n",
       "      <td>-27.620451</td>\n",
       "      <td>-27.888905</td>\n",
       "      <td>-28.871309</td>\n",
       "      <td>-27.902435</td>\n",
       "      <td>-28.278313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-23.589674</td>\n",
       "      <td>-24.579994</td>\n",
       "      <td>-22.594236</td>\n",
       "      <td>-21.481213</td>\n",
       "      <td>-20.949923</td>\n",
       "      <td>-20.414589</td>\n",
       "      <td>-20.267546</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.499668</td>\n",
       "      <td>-10.080903</td>\n",
       "      <td>-12.700766</td>\n",
       "      <td>-17.040066</td>\n",
       "      <td>-20.240370</td>\n",
       "      <td>-23.302591</td>\n",
       "      <td>-24.621037</td>\n",
       "      <td>-23.829395</td>\n",
       "      <td>-12.847005</td>\n",
       "      <td>-5.907684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>male_disgust</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-22.956581</td>\n",
       "      <td>-23.019234</td>\n",
       "      <td>-20.557930</td>\n",
       "      <td>-19.553026</td>\n",
       "      <td>-22.532879</td>\n",
       "      <td>-23.454952</td>\n",
       "      <td>-21.624464</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.982821</td>\n",
       "      <td>-18.645363</td>\n",
       "      <td>-21.921246</td>\n",
       "      <td>-23.019463</td>\n",
       "      <td>-21.649454</td>\n",
       "      <td>-23.983215</td>\n",
       "      <td>-24.639437</td>\n",
       "      <td>-26.931631</td>\n",
       "      <td>-27.579979</td>\n",
       "      <td>-27.450191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source                                               path  \\\n",
       "0   male_neutral  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "1      male_fear  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "2  male_surprise  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "3       male_sad  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "4   male_disgust  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "\n",
       "           0          1          2          3          4          5  \\\n",
       "0  -3.008258  -5.508383 -11.619889 -11.603205 -13.251027 -12.154624   \n",
       "1 -10.467623  -8.993829 -11.776381 -13.744241 -15.344474 -15.923333   \n",
       "2 -12.981858  -8.290445  -9.102726 -10.725917 -10.041571 -13.318968   \n",
       "3 -23.589674 -24.579994 -22.594236 -21.481213 -20.949923 -20.414589   \n",
       "4 -22.956581 -23.019234 -20.557930 -19.553026 -22.532879 -23.454952   \n",
       "\n",
       "           6  ...        206        207        208        209        210  \\\n",
       "0 -12.164827  ... -28.975243 -27.885740 -27.087734 -29.094940 -26.330254   \n",
       "1 -13.486449  ...  -8.035086  -7.168881  -6.481255  -6.759146  -6.833916   \n",
       "2 -15.079294  ... -25.084118 -26.953909 -28.918743 -28.394785 -26.758858   \n",
       "3 -20.267546  ...  -8.499668 -10.080903 -12.700766 -17.040066 -20.240370   \n",
       "4 -21.624464  ... -17.982821 -18.645363 -21.921246 -23.019463 -21.649454   \n",
       "\n",
       "         211        212        213        214        215  \n",
       "0 -25.438536 -25.739513 -26.417332 -25.851837 -25.928373  \n",
       "1  -6.347439  -5.493447  -5.551433  -0.891208   2.769547  \n",
       "2 -27.620451 -27.888905 -28.871309 -27.902435 -28.278313  \n",
       "3 -23.302591 -24.621037 -23.829395 -12.847005  -5.907684  \n",
       "4 -23.983215 -24.639437 -26.931631 -27.579979 -27.450191  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now extract the mean bands to its own feature columns\n",
    "final_df = pd.concat([df,pd.DataFrame(feature_df['feature'].values.tolist())],axis=1)\n",
    "final_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12162, 219)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>source</th>\n",
       "      <th>path</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>male_neutral</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-3.008258</td>\n",
       "      <td>-5.508383</td>\n",
       "      <td>-11.619889</td>\n",
       "      <td>-11.603205</td>\n",
       "      <td>-13.251027</td>\n",
       "      <td>-12.154624</td>\n",
       "      <td>-12.164827</td>\n",
       "      <td>...</td>\n",
       "      <td>-28.975243</td>\n",
       "      <td>-27.885740</td>\n",
       "      <td>-27.087734</td>\n",
       "      <td>-29.094940</td>\n",
       "      <td>-26.330254</td>\n",
       "      <td>-25.438536</td>\n",
       "      <td>-25.739513</td>\n",
       "      <td>-26.417332</td>\n",
       "      <td>-25.851837</td>\n",
       "      <td>-25.928373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>male_fear</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-10.467623</td>\n",
       "      <td>-8.993829</td>\n",
       "      <td>-11.776381</td>\n",
       "      <td>-13.744241</td>\n",
       "      <td>-15.344474</td>\n",
       "      <td>-15.923333</td>\n",
       "      <td>-13.486449</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.035086</td>\n",
       "      <td>-7.168881</td>\n",
       "      <td>-6.481255</td>\n",
       "      <td>-6.759146</td>\n",
       "      <td>-6.833916</td>\n",
       "      <td>-6.347439</td>\n",
       "      <td>-5.493447</td>\n",
       "      <td>-5.551433</td>\n",
       "      <td>-0.891208</td>\n",
       "      <td>2.769547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>male_surprise</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-12.981858</td>\n",
       "      <td>-8.290445</td>\n",
       "      <td>-9.102726</td>\n",
       "      <td>-10.725917</td>\n",
       "      <td>-10.041571</td>\n",
       "      <td>-13.318968</td>\n",
       "      <td>-15.079294</td>\n",
       "      <td>...</td>\n",
       "      <td>-25.084118</td>\n",
       "      <td>-26.953909</td>\n",
       "      <td>-28.918743</td>\n",
       "      <td>-28.394785</td>\n",
       "      <td>-26.758858</td>\n",
       "      <td>-27.620451</td>\n",
       "      <td>-27.888905</td>\n",
       "      <td>-28.871309</td>\n",
       "      <td>-27.902435</td>\n",
       "      <td>-28.278313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>male_sad</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-23.589674</td>\n",
       "      <td>-24.579994</td>\n",
       "      <td>-22.594236</td>\n",
       "      <td>-21.481213</td>\n",
       "      <td>-20.949923</td>\n",
       "      <td>-20.414589</td>\n",
       "      <td>-20.267546</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.499668</td>\n",
       "      <td>-10.080903</td>\n",
       "      <td>-12.700766</td>\n",
       "      <td>-17.040066</td>\n",
       "      <td>-20.240370</td>\n",
       "      <td>-23.302591</td>\n",
       "      <td>-24.621037</td>\n",
       "      <td>-23.829395</td>\n",
       "      <td>-12.847005</td>\n",
       "      <td>-5.907684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>male_disgust</td>\n",
       "      <td>SAVEE</td>\n",
       "      <td>/home/bukya/Desktop/Speech_Emotion_Recognition...</td>\n",
       "      <td>-22.956581</td>\n",
       "      <td>-23.019234</td>\n",
       "      <td>-20.557930</td>\n",
       "      <td>-19.553026</td>\n",
       "      <td>-22.532879</td>\n",
       "      <td>-23.454952</td>\n",
       "      <td>-21.624464</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.982821</td>\n",
       "      <td>-18.645363</td>\n",
       "      <td>-21.921246</td>\n",
       "      <td>-23.019463</td>\n",
       "      <td>-21.649454</td>\n",
       "      <td>-23.983215</td>\n",
       "      <td>-24.639437</td>\n",
       "      <td>-26.931631</td>\n",
       "      <td>-27.579979</td>\n",
       "      <td>-27.450191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          labels source                                               path  \\\n",
       "0   male_neutral  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "1      male_fear  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "2  male_surprise  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "3       male_sad  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "4   male_disgust  SAVEE  /home/bukya/Desktop/Speech_Emotion_Recognition...   \n",
       "\n",
       "           0          1          2          3          4          5  \\\n",
       "0  -3.008258  -5.508383 -11.619889 -11.603205 -13.251027 -12.154624   \n",
       "1 -10.467623  -8.993829 -11.776381 -13.744241 -15.344474 -15.923333   \n",
       "2 -12.981858  -8.290445  -9.102726 -10.725917 -10.041571 -13.318968   \n",
       "3 -23.589674 -24.579994 -22.594236 -21.481213 -20.949923 -20.414589   \n",
       "4 -22.956581 -23.019234 -20.557930 -19.553026 -22.532879 -23.454952   \n",
       "\n",
       "           6  ...        206        207        208        209        210  \\\n",
       "0 -12.164827  ... -28.975243 -27.885740 -27.087734 -29.094940 -26.330254   \n",
       "1 -13.486449  ...  -8.035086  -7.168881  -6.481255  -6.759146  -6.833916   \n",
       "2 -15.079294  ... -25.084118 -26.953909 -28.918743 -28.394785 -26.758858   \n",
       "3 -20.267546  ...  -8.499668 -10.080903 -12.700766 -17.040066 -20.240370   \n",
       "4 -21.624464  ... -17.982821 -18.645363 -21.921246 -23.019463 -21.649454   \n",
       "\n",
       "         211        212        213        214        215  \n",
       "0 -25.438536 -25.739513 -26.417332 -25.851837 -25.928373  \n",
       "1  -6.347439  -5.493447  -5.551433  -0.891208   2.769547  \n",
       "2 -27.620451 -27.888905 -28.871309 -27.902435 -28.278313  \n",
       "3 -23.302591 -24.621037 -23.829395 -12.847005  -5.907684  \n",
       "4 -23.983215 -24.639437 -26.931631 -27.579979 -27.450191  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace NA with 0\n",
    "final_df=final_df.fillna(0)\n",
    "print(final_df.shape)\n",
    "final_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>-18.611179</td>\n",
       "      <td>-17.616539</td>\n",
       "      <td>-18.411484</td>\n",
       "      <td>-18.987419</td>\n",
       "      <td>-17.404621</td>\n",
       "      <td>-16.747272</td>\n",
       "      <td>-17.733747</td>\n",
       "      <td>-18.055025</td>\n",
       "      <td>-17.931210</td>\n",
       "      <td>-15.913172</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.899403</td>\n",
       "      <td>-21.647816</td>\n",
       "      <td>-19.758656</td>\n",
       "      <td>-18.879402</td>\n",
       "      <td>-19.397377</td>\n",
       "      <td>-20.171659</td>\n",
       "      <td>-22.689243</td>\n",
       "      <td>-24.612814</td>\n",
       "      <td>-24.153776</td>\n",
       "      <td>-22.703135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>-23.275175</td>\n",
       "      <td>-22.550425</td>\n",
       "      <td>-18.601215</td>\n",
       "      <td>-19.310427</td>\n",
       "      <td>-19.504612</td>\n",
       "      <td>-20.461906</td>\n",
       "      <td>-23.478741</td>\n",
       "      <td>-25.597187</td>\n",
       "      <td>-28.436279</td>\n",
       "      <td>-27.643888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9761</td>\n",
       "      <td>-1.533947</td>\n",
       "      <td>-4.030602</td>\n",
       "      <td>-9.614023</td>\n",
       "      <td>-12.045173</td>\n",
       "      <td>-9.992992</td>\n",
       "      <td>-11.926250</td>\n",
       "      <td>-14.008465</td>\n",
       "      <td>-13.561555</td>\n",
       "      <td>-14.024568</td>\n",
       "      <td>-15.151947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7620</td>\n",
       "      <td>-4.531077</td>\n",
       "      <td>-3.933792</td>\n",
       "      <td>-4.567834</td>\n",
       "      <td>-5.871509</td>\n",
       "      <td>-5.282475</td>\n",
       "      <td>-6.490459</td>\n",
       "      <td>-8.156466</td>\n",
       "      <td>-9.188803</td>\n",
       "      <td>-8.681725</td>\n",
       "      <td>-8.212409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11586</td>\n",
       "      <td>-20.621702</td>\n",
       "      <td>-21.587507</td>\n",
       "      <td>-20.563646</td>\n",
       "      <td>-20.703459</td>\n",
       "      <td>-21.205715</td>\n",
       "      <td>-18.608534</td>\n",
       "      <td>-18.446669</td>\n",
       "      <td>-16.211845</td>\n",
       "      <td>-14.257651</td>\n",
       "      <td>-15.160404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7914</td>\n",
       "      <td>-17.514988</td>\n",
       "      <td>-18.551867</td>\n",
       "      <td>-17.043016</td>\n",
       "      <td>-16.977903</td>\n",
       "      <td>-19.369633</td>\n",
       "      <td>-19.562126</td>\n",
       "      <td>-22.008749</td>\n",
       "      <td>-20.178385</td>\n",
       "      <td>-17.989597</td>\n",
       "      <td>-19.336285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9513</td>\n",
       "      <td>-18.740368</td>\n",
       "      <td>-18.824930</td>\n",
       "      <td>-16.149488</td>\n",
       "      <td>-16.963457</td>\n",
       "      <td>-18.229979</td>\n",
       "      <td>-18.183952</td>\n",
       "      <td>-19.274342</td>\n",
       "      <td>-18.395123</td>\n",
       "      <td>-16.951286</td>\n",
       "      <td>-16.672031</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.882130</td>\n",
       "      <td>-19.390713</td>\n",
       "      <td>-17.779472</td>\n",
       "      <td>-19.165974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5835</td>\n",
       "      <td>-19.066849</td>\n",
       "      <td>-18.328381</td>\n",
       "      <td>-17.710285</td>\n",
       "      <td>-18.043192</td>\n",
       "      <td>-18.252480</td>\n",
       "      <td>-18.710625</td>\n",
       "      <td>-16.626352</td>\n",
       "      <td>-17.831005</td>\n",
       "      <td>-18.028343</td>\n",
       "      <td>-17.859104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5389</td>\n",
       "      <td>-20.760590</td>\n",
       "      <td>-20.047138</td>\n",
       "      <td>-18.961346</td>\n",
       "      <td>-19.468687</td>\n",
       "      <td>-19.316292</td>\n",
       "      <td>-18.162563</td>\n",
       "      <td>-18.102333</td>\n",
       "      <td>-19.914133</td>\n",
       "      <td>-20.931385</td>\n",
       "      <td>-19.215496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11222</td>\n",
       "      <td>-18.252924</td>\n",
       "      <td>-17.727373</td>\n",
       "      <td>-19.222475</td>\n",
       "      <td>-18.469971</td>\n",
       "      <td>-17.572325</td>\n",
       "      <td>-17.850542</td>\n",
       "      <td>-17.932026</td>\n",
       "      <td>-20.588900</td>\n",
       "      <td>-18.612183</td>\n",
       "      <td>-15.990726</td>\n",
       "      <td>...</td>\n",
       "      <td>-18.065437</td>\n",
       "      <td>-18.135090</td>\n",
       "      <td>-19.665306</td>\n",
       "      <td>-20.741905</td>\n",
       "      <td>-20.273037</td>\n",
       "      <td>-18.371035</td>\n",
       "      <td>-15.576723</td>\n",
       "      <td>-17.512489</td>\n",
       "      <td>-17.008547</td>\n",
       "      <td>-18.195284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0          1          2          3          4          5    \\\n",
       "4950  -18.611179 -17.616539 -18.411484 -18.987419 -17.404621 -16.747272   \n",
       "3860  -23.275175 -22.550425 -18.601215 -19.310427 -19.504612 -20.461906   \n",
       "9761   -1.533947  -4.030602  -9.614023 -12.045173  -9.992992 -11.926250   \n",
       "7620   -4.531077  -3.933792  -4.567834  -5.871509  -5.282475  -6.490459   \n",
       "11586 -20.621702 -21.587507 -20.563646 -20.703459 -21.205715 -18.608534   \n",
       "7914  -17.514988 -18.551867 -17.043016 -16.977903 -19.369633 -19.562126   \n",
       "9513  -18.740368 -18.824930 -16.149488 -16.963457 -18.229979 -18.183952   \n",
       "5835  -19.066849 -18.328381 -17.710285 -18.043192 -18.252480 -18.710625   \n",
       "5389  -20.760590 -20.047138 -18.961346 -19.468687 -19.316292 -18.162563   \n",
       "11222 -18.252924 -17.727373 -19.222475 -18.469971 -17.572325 -17.850542   \n",
       "\n",
       "             6          7          8          9    ...        206        207  \\\n",
       "4950  -17.733747 -18.055025 -17.931210 -15.913172  ... -22.899403 -21.647816   \n",
       "3860  -23.478741 -25.597187 -28.436279 -27.643888  ...   0.000000   0.000000   \n",
       "9761  -14.008465 -13.561555 -14.024568 -15.151947  ...   0.000000   0.000000   \n",
       "7620   -8.156466  -9.188803  -8.681725  -8.212409  ...   0.000000   0.000000   \n",
       "11586 -18.446669 -16.211845 -14.257651 -15.160404  ...   0.000000   0.000000   \n",
       "7914  -22.008749 -20.178385 -17.989597 -19.336285  ...   0.000000   0.000000   \n",
       "9513  -19.274342 -18.395123 -16.951286 -16.672031  ... -17.882130 -19.390713   \n",
       "5835  -16.626352 -17.831005 -18.028343 -17.859104  ...   0.000000   0.000000   \n",
       "5389  -18.102333 -19.914133 -20.931385 -19.215496  ...   0.000000   0.000000   \n",
       "11222 -17.932026 -20.588900 -18.612183 -15.990726  ... -18.065437 -18.135090   \n",
       "\n",
       "             208        209        210        211        212        213  \\\n",
       "4950  -19.758656 -18.879402 -19.397377 -20.171659 -22.689243 -24.612814   \n",
       "3860    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9761    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7620    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11586   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "7914    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "9513  -17.779472 -19.165974   0.000000   0.000000   0.000000   0.000000   \n",
       "5835    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "5389    0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11222 -19.665306 -20.741905 -20.273037 -18.371035 -15.576723 -17.512489   \n",
       "\n",
       "             214        215  \n",
       "4950  -24.153776 -22.703135  \n",
       "3860    0.000000   0.000000  \n",
       "9761    0.000000   0.000000  \n",
       "7620    0.000000   0.000000  \n",
       "11586   0.000000   0.000000  \n",
       "7914    0.000000   0.000000  \n",
       "9513    0.000000   0.000000  \n",
       "5835    0.000000   0.000000  \n",
       "5389    0.000000   0.000000  \n",
       "11222 -17.008547 -18.195284  \n",
       "\n",
       "[10 rows x 216 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(final_df.drop(['path','labels','source'],axis=1)\n",
    "                                                    , final_df.labels\n",
    "                                                    , test_size=0.25\n",
    "                                                    , shuffle=True\n",
    "                                                    , random_state=42\n",
    "                                                   )\n",
    "\n",
    "# Lets see how the data present itself before normalisation \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>0.185439</td>\n",
       "      <td>0.302201</td>\n",
       "      <td>0.437356</td>\n",
       "      <td>0.388014</td>\n",
       "      <td>0.497720</td>\n",
       "      <td>0.540960</td>\n",
       "      <td>0.459522</td>\n",
       "      <td>0.431126</td>\n",
       "      <td>0.434075</td>\n",
       "      <td>0.579169</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.871456</td>\n",
       "      <td>-0.803882</td>\n",
       "      <td>-0.685581</td>\n",
       "      <td>-0.624922</td>\n",
       "      <td>-0.671214</td>\n",
       "      <td>-0.713581</td>\n",
       "      <td>-0.856821</td>\n",
       "      <td>-0.978781</td>\n",
       "      <td>-0.961893</td>\n",
       "      <td>-0.871796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>-0.139970</td>\n",
       "      <td>-0.055768</td>\n",
       "      <td>0.423287</td>\n",
       "      <td>0.364109</td>\n",
       "      <td>0.342614</td>\n",
       "      <td>0.266639</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>-0.123282</td>\n",
       "      <td>-0.338554</td>\n",
       "      <td>-0.283990</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9761</td>\n",
       "      <td>1.376928</td>\n",
       "      <td>1.287903</td>\n",
       "      <td>1.089695</td>\n",
       "      <td>0.901790</td>\n",
       "      <td>1.045145</td>\n",
       "      <td>0.896987</td>\n",
       "      <td>0.733797</td>\n",
       "      <td>0.761431</td>\n",
       "      <td>0.721401</td>\n",
       "      <td>0.635181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7620</td>\n",
       "      <td>1.167816</td>\n",
       "      <td>1.294927</td>\n",
       "      <td>1.463874</td>\n",
       "      <td>1.358686</td>\n",
       "      <td>1.393065</td>\n",
       "      <td>1.298414</td>\n",
       "      <td>1.164653</td>\n",
       "      <td>1.082863</td>\n",
       "      <td>1.114358</td>\n",
       "      <td>1.145800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11586</td>\n",
       "      <td>0.045164</td>\n",
       "      <td>0.014095</td>\n",
       "      <td>0.277771</td>\n",
       "      <td>0.261014</td>\n",
       "      <td>0.216970</td>\n",
       "      <td>0.403508</td>\n",
       "      <td>0.407033</td>\n",
       "      <td>0.566614</td>\n",
       "      <td>0.704259</td>\n",
       "      <td>0.634559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7914</td>\n",
       "      <td>0.261921</td>\n",
       "      <td>0.234340</td>\n",
       "      <td>0.538829</td>\n",
       "      <td>0.536732</td>\n",
       "      <td>0.352584</td>\n",
       "      <td>0.333086</td>\n",
       "      <td>0.144774</td>\n",
       "      <td>0.275043</td>\n",
       "      <td>0.429781</td>\n",
       "      <td>0.327293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9513</td>\n",
       "      <td>0.176426</td>\n",
       "      <td>0.214528</td>\n",
       "      <td>0.605085</td>\n",
       "      <td>0.537801</td>\n",
       "      <td>0.436759</td>\n",
       "      <td>0.434863</td>\n",
       "      <td>0.346095</td>\n",
       "      <td>0.406126</td>\n",
       "      <td>0.506146</td>\n",
       "      <td>0.523332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562113</td>\n",
       "      <td>-0.665129</td>\n",
       "      <td>-0.564183</td>\n",
       "      <td>-0.642408</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5835</td>\n",
       "      <td>0.153647</td>\n",
       "      <td>0.250555</td>\n",
       "      <td>0.489350</td>\n",
       "      <td>0.457893</td>\n",
       "      <td>0.435097</td>\n",
       "      <td>0.395969</td>\n",
       "      <td>0.541054</td>\n",
       "      <td>0.447593</td>\n",
       "      <td>0.426931</td>\n",
       "      <td>0.435985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5389</td>\n",
       "      <td>0.035474</td>\n",
       "      <td>0.125853</td>\n",
       "      <td>0.396583</td>\n",
       "      <td>0.352396</td>\n",
       "      <td>0.356523</td>\n",
       "      <td>0.436443</td>\n",
       "      <td>0.432385</td>\n",
       "      <td>0.294467</td>\n",
       "      <td>0.213417</td>\n",
       "      <td>0.336180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.540423</td>\n",
       "      <td>0.526891</td>\n",
       "      <td>0.526358</td>\n",
       "      <td>0.527078</td>\n",
       "      <td>0.512020</td>\n",
       "      <td>0.512089</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.496370</td>\n",
       "      <td>0.491865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11222</td>\n",
       "      <td>0.210435</td>\n",
       "      <td>0.294160</td>\n",
       "      <td>0.377220</td>\n",
       "      <td>0.426308</td>\n",
       "      <td>0.485333</td>\n",
       "      <td>0.459485</td>\n",
       "      <td>0.444924</td>\n",
       "      <td>0.244867</td>\n",
       "      <td>0.383991</td>\n",
       "      <td>0.573463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.573415</td>\n",
       "      <td>-0.587941</td>\n",
       "      <td>-0.679855</td>\n",
       "      <td>-0.738570</td>\n",
       "      <td>-0.724629</td>\n",
       "      <td>-0.604171</td>\n",
       "      <td>-0.427447</td>\n",
       "      <td>-0.551805</td>\n",
       "      <td>-0.530506</td>\n",
       "      <td>-0.601032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "4950   0.185439  0.302201  0.437356  0.388014  0.497720  0.540960  0.459522   \n",
       "3860  -0.139970 -0.055768  0.423287  0.364109  0.342614  0.266639  0.036545   \n",
       "9761   1.376928  1.287903  1.089695  0.901790  1.045145  0.896987  0.733797   \n",
       "7620   1.167816  1.294927  1.463874  1.358686  1.393065  1.298414  1.164653   \n",
       "11586  0.045164  0.014095  0.277771  0.261014  0.216970  0.403508  0.407033   \n",
       "7914   0.261921  0.234340  0.538829  0.536732  0.352584  0.333086  0.144774   \n",
       "9513   0.176426  0.214528  0.605085  0.537801  0.436759  0.434863  0.346095   \n",
       "5835   0.153647  0.250555  0.489350  0.457893  0.435097  0.395969  0.541054   \n",
       "5389   0.035474  0.125853  0.396583  0.352396  0.356523  0.436443  0.432385   \n",
       "11222  0.210435  0.294160  0.377220  0.426308  0.485333  0.459485  0.444924   \n",
       "\n",
       "            7         8         9    ...       206       207       208  \\\n",
       "4950   0.431126  0.434075  0.579169  ... -0.871456 -0.803882 -0.685581   \n",
       "3860  -0.123282 -0.338554 -0.283990  ...  0.540423  0.526891  0.526358   \n",
       "9761   0.761431  0.721401  0.635181  ...  0.540423  0.526891  0.526358   \n",
       "7620   1.082863  1.114358  1.145800  ...  0.540423  0.526891  0.526358   \n",
       "11586  0.566614  0.704259  0.634559  ...  0.540423  0.526891  0.526358   \n",
       "7914   0.275043  0.429781  0.327293  ...  0.540423  0.526891  0.526358   \n",
       "9513   0.406126  0.506146  0.523332  ... -0.562113 -0.665129 -0.564183   \n",
       "5835   0.447593  0.426931  0.435985  ...  0.540423  0.526891  0.526358   \n",
       "5389   0.294467  0.213417  0.336180  ...  0.540423  0.526891  0.526358   \n",
       "11222  0.244867  0.383991  0.573463  ... -0.573415 -0.587941 -0.679855   \n",
       "\n",
       "            209       210       211       212       213       214       215  \n",
       "4950  -0.624922 -0.671214 -0.713581 -0.856821 -0.978781 -0.961893 -0.871796  \n",
       "3860   0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "9761   0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "7620   0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "11586  0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "7914   0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "9513  -0.642408  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "5835   0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "5389   0.527078  0.512020  0.512089  0.512900  0.501302  0.496370  0.491865  \n",
       "11222 -0.738570 -0.724629 -0.604171 -0.427447 -0.551805 -0.530506 -0.601032  \n",
       "\n",
       "[10 rows x 216 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets do data normalization \n",
    "mean = np.mean(X_train, axis=0)\n",
    "std = np.std(X_train, axis=0)\n",
    "\n",
    "X_train = (X_train - mean)/std\n",
    "X_test = (X_test - mean)/std\n",
    "\n",
    "# Check the dataset now \n",
    "X_train[150:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9121, 216)\n",
      "['female_angry' 'female_disgust' 'female_fear' 'female_happy'\n",
      " 'female_neutral' 'female_sad' 'female_surprise' 'male_angry'\n",
      " 'male_disgust' 'male_fear' 'male_happy' 'male_neutral' 'male_sad'\n",
      " 'male_surprise']\n"
     ]
    }
   ],
   "source": [
    "# Lets few preparation steps to get it into the correct format for Keras \n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# one hot encode the target \n",
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(lb.classes_)\n",
    "#print(y_train[0:10])\n",
    "#print(y_test[0:10])\n",
    "\n",
    "# Pickel the lb object for future use \n",
    "filename = 'labels'\n",
    "outfile = open(filename,'wb')\n",
    "pickle.dump(lb,outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9121, 216, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 216, 256)          2304      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 216, 256)          524544    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 216, 256)          1024      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 27, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 27, 128)           262272    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 27, 128)           131200    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 27, 128)           131200    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 27, 128)           131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 27, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 3, 64)             65600     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 3, 64)             32832     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 3, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                2702      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 1,285,390\n",
      "Trainable params: 1,284,622\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# CNN model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(256, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 8, padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(64, 8, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(14)) # Target class number\n",
    "model.add(Activation('softmax'))\n",
    "# opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)\n",
    "# opt = keras.optimizers.Adam(lr=0.0001)\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9121 samples, validate on 3041 samples\n",
      "Epoch 1/100\n",
      "9121/9121 [==============================] - 158s 17ms/step - loss: 2.4082 - accuracy: 0.1835 - val_loss: 2.4494 - val_accuracy: 0.2144\n",
      "Epoch 2/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 2.1931 - accuracy: 0.2490 - val_loss: 2.2398 - val_accuracy: 0.2667\n",
      "Epoch 3/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 2.0861 - accuracy: 0.2898 - val_loss: 2.1441 - val_accuracy: 0.2986\n",
      "Epoch 4/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 2.0133 - accuracy: 0.3076 - val_loss: 2.0924 - val_accuracy: 0.3200\n",
      "Epoch 5/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.9583 - accuracy: 0.3293 - val_loss: 2.0581 - val_accuracy: 0.3242\n",
      "Epoch 6/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.9151 - accuracy: 0.3416 - val_loss: 1.9962 - val_accuracy: 0.3630\n",
      "Epoch 7/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.8770 - accuracy: 0.3598 - val_loss: 1.9758 - val_accuracy: 0.3630\n",
      "Epoch 8/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.8433 - accuracy: 0.3685 - val_loss: 1.9329 - val_accuracy: 0.3821\n",
      "Epoch 9/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.8090 - accuracy: 0.3836 - val_loss: 1.9147 - val_accuracy: 0.3847\n",
      "Epoch 10/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.7819 - accuracy: 0.3886 - val_loss: 1.8731 - val_accuracy: 0.3897\n",
      "Epoch 11/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.7525 - accuracy: 0.3991 - val_loss: 1.8680 - val_accuracy: 0.3916\n",
      "Epoch 12/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.7306 - accuracy: 0.4019 - val_loss: 1.8519 - val_accuracy: 0.3979\n",
      "Epoch 13/100\n",
      "9121/9121 [==============================] - 153s 17ms/step - loss: 1.7103 - accuracy: 0.4129 - val_loss: 1.8321 - val_accuracy: 0.3949\n",
      "Epoch 14/100\n",
      "9121/9121 [==============================] - 158s 17ms/step - loss: 1.6869 - accuracy: 0.4151 - val_loss: 1.8172 - val_accuracy: 0.3959\n",
      "Epoch 15/100\n",
      "9121/9121 [==============================] - 168s 18ms/step - loss: 1.6671 - accuracy: 0.4282 - val_loss: 1.7918 - val_accuracy: 0.4107\n",
      "Epoch 16/100\n",
      "9121/9121 [==============================] - 181s 20ms/step - loss: 1.6506 - accuracy: 0.4334 - val_loss: 1.7942 - val_accuracy: 0.3976\n",
      "Epoch 17/100\n",
      "9121/9121 [==============================] - 190s 21ms/step - loss: 1.6336 - accuracy: 0.4390 - val_loss: 1.7633 - val_accuracy: 0.4212\n",
      "Epoch 18/100\n",
      "9121/9121 [==============================] - 187s 21ms/step - loss: 1.6171 - accuracy: 0.4458 - val_loss: 1.7540 - val_accuracy: 0.4226\n",
      "Epoch 19/100\n",
      "9121/9121 [==============================] - 184s 20ms/step - loss: 1.6071 - accuracy: 0.4417 - val_loss: 1.7367 - val_accuracy: 0.4360\n",
      "Epoch 20/100\n",
      "9121/9121 [==============================] - 186s 20ms/step - loss: 1.5908 - accuracy: 0.4482 - val_loss: 1.7529 - val_accuracy: 0.4130\n",
      "Epoch 21/100\n",
      "9121/9121 [==============================] - 186s 20ms/step - loss: 1.5722 - accuracy: 0.4512 - val_loss: 1.7261 - val_accuracy: 0.4258\n",
      "Epoch 22/100\n",
      "9121/9121 [==============================] - 192s 21ms/step - loss: 1.5666 - accuracy: 0.4597 - val_loss: 1.7349 - val_accuracy: 0.4127\n",
      "Epoch 23/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.5482 - accuracy: 0.4664 - val_loss: 1.7415 - val_accuracy: 0.4235\n",
      "Epoch 24/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.5393 - accuracy: 0.4671 - val_loss: 1.6986 - val_accuracy: 0.4291\n",
      "Epoch 25/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.5262 - accuracy: 0.4724 - val_loss: 1.6987 - val_accuracy: 0.4318\n",
      "Epoch 26/100\n",
      "9121/9121 [==============================] - 179s 20ms/step - loss: 1.5156 - accuracy: 0.4718 - val_loss: 1.6795 - val_accuracy: 0.4466\n",
      "Epoch 27/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.5057 - accuracy: 0.4799 - val_loss: 1.6781 - val_accuracy: 0.4334\n",
      "Epoch 28/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.4965 - accuracy: 0.4839 - val_loss: 1.6815 - val_accuracy: 0.4334\n",
      "Epoch 29/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.4840 - accuracy: 0.4882 - val_loss: 1.6617 - val_accuracy: 0.4410\n",
      "Epoch 30/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.4760 - accuracy: 0.4899 - val_loss: 1.6690 - val_accuracy: 0.4357\n",
      "Epoch 31/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.4706 - accuracy: 0.4941 - val_loss: 1.6517 - val_accuracy: 0.4403\n",
      "Epoch 32/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.4521 - accuracy: 0.4967 - val_loss: 1.6583 - val_accuracy: 0.4502\n",
      "Epoch 33/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.4460 - accuracy: 0.5003 - val_loss: 1.6509 - val_accuracy: 0.4397\n",
      "Epoch 34/100\n",
      "9121/9121 [==============================] - 187s 20ms/step - loss: 1.4379 - accuracy: 0.5024 - val_loss: 1.6644 - val_accuracy: 0.4311\n",
      "Epoch 35/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.4293 - accuracy: 0.5072 - val_loss: 1.6469 - val_accuracy: 0.4436\n",
      "Epoch 36/100\n",
      "9121/9121 [==============================] - 194s 21ms/step - loss: 1.4193 - accuracy: 0.5126 - val_loss: 1.6462 - val_accuracy: 0.4357\n",
      "Epoch 37/100\n",
      "9121/9121 [==============================] - 192s 21ms/step - loss: 1.4119 - accuracy: 0.5072 - val_loss: 1.6336 - val_accuracy: 0.4522\n",
      "Epoch 38/100\n",
      "9121/9121 [==============================] - 191s 21ms/step - loss: 1.3947 - accuracy: 0.5161 - val_loss: 1.6384 - val_accuracy: 0.4397\n",
      "Epoch 39/100\n",
      "9121/9121 [==============================] - 191s 21ms/step - loss: 1.3916 - accuracy: 0.5202 - val_loss: 1.6326 - val_accuracy: 0.4512\n",
      "Epoch 40/100\n",
      "9121/9121 [==============================] - 190s 21ms/step - loss: 1.3806 - accuracy: 0.5274 - val_loss: 1.6246 - val_accuracy: 0.4397\n",
      "Epoch 41/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.3782 - accuracy: 0.5240 - val_loss: 1.6296 - val_accuracy: 0.4416\n",
      "Epoch 42/100\n",
      "9121/9121 [==============================] - 187s 21ms/step - loss: 1.3706 - accuracy: 0.5291 - val_loss: 1.6097 - val_accuracy: 0.4410\n",
      "Epoch 43/100\n",
      "9121/9121 [==============================] - 187s 20ms/step - loss: 1.3572 - accuracy: 0.5315 - val_loss: 1.6079 - val_accuracy: 0.4452\n",
      "Epoch 44/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.3486 - accuracy: 0.5338 - val_loss: 1.6075 - val_accuracy: 0.4508\n",
      "Epoch 45/100\n",
      "9121/9121 [==============================] - 189s 21ms/step - loss: 1.3399 - accuracy: 0.5358 - val_loss: 1.6130 - val_accuracy: 0.4456\n",
      "Epoch 46/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.3348 - accuracy: 0.5480 - val_loss: 1.6675 - val_accuracy: 0.4281\n",
      "Epoch 47/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.3220 - accuracy: 0.5464 - val_loss: 1.6102 - val_accuracy: 0.4400\n",
      "Epoch 48/100\n",
      "9121/9121 [==============================] - 187s 21ms/step - loss: 1.3189 - accuracy: 0.5476 - val_loss: 1.6224 - val_accuracy: 0.4357\n",
      "Epoch 49/100\n",
      "9121/9121 [==============================] - 188s 21ms/step - loss: 1.3080 - accuracy: 0.5550 - val_loss: 1.6259 - val_accuracy: 0.4318\n",
      "Epoch 50/100\n",
      "9121/9121 [==============================] - 180s 20ms/step - loss: 1.3001 - accuracy: 0.5511 - val_loss: 1.5970 - val_accuracy: 0.4469\n",
      "Epoch 51/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.2831 - accuracy: 0.5613 - val_loss: 1.6129 - val_accuracy: 0.4413\n",
      "Epoch 52/100\n",
      "9121/9121 [==============================] - 159s 17ms/step - loss: 1.2842 - accuracy: 0.5616 - val_loss: 1.6190 - val_accuracy: 0.4383\n",
      "Epoch 53/100\n",
      "9121/9121 [==============================] - 166s 18ms/step - loss: 1.2760 - accuracy: 0.5647 - val_loss: 1.6339 - val_accuracy: 0.4344\n",
      "Epoch 54/100\n",
      "9121/9121 [==============================] - 172s 19ms/step - loss: 1.2650 - accuracy: 0.5693 - val_loss: 1.6175 - val_accuracy: 0.4449\n",
      "Epoch 55/100\n",
      "9121/9121 [==============================] - 173s 19ms/step - loss: 1.2609 - accuracy: 0.5661 - val_loss: 1.5953 - val_accuracy: 0.4482\n",
      "Epoch 56/100\n",
      "9121/9121 [==============================] - 174s 19ms/step - loss: 1.2569 - accuracy: 0.5697 - val_loss: 1.5865 - val_accuracy: 0.4462\n",
      "Epoch 57/100\n",
      "9121/9121 [==============================] - 163s 18ms/step - loss: 1.2426 - accuracy: 0.5802 - val_loss: 1.5863 - val_accuracy: 0.4577\n",
      "Epoch 58/100\n",
      "9121/9121 [==============================] - 148s 16ms/step - loss: 1.2391 - accuracy: 0.5769 - val_loss: 1.5893 - val_accuracy: 0.4459\n",
      "Epoch 59/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.2295 - accuracy: 0.5811 - val_loss: 1.6031 - val_accuracy: 0.4476\n",
      "Epoch 60/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.2142 - accuracy: 0.5894 - val_loss: 1.5893 - val_accuracy: 0.4380\n",
      "Epoch 61/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.2113 - accuracy: 0.5881 - val_loss: 1.5920 - val_accuracy: 0.4429\n",
      "Epoch 62/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.2104 - accuracy: 0.5889 - val_loss: 1.5723 - val_accuracy: 0.4433\n",
      "Epoch 63/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.1954 - accuracy: 0.5947 - val_loss: 1.5712 - val_accuracy: 0.4446\n",
      "Epoch 64/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.1900 - accuracy: 0.5951 - val_loss: 1.5910 - val_accuracy: 0.4403\n",
      "Epoch 65/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.1819 - accuracy: 0.6023 - val_loss: 1.6083 - val_accuracy: 0.4354\n",
      "Epoch 66/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.1744 - accuracy: 0.6008 - val_loss: 1.6016 - val_accuracy: 0.4433\n",
      "Epoch 67/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.1667 - accuracy: 0.6009 - val_loss: 1.5897 - val_accuracy: 0.4433\n",
      "Epoch 68/100\n",
      "9121/9121 [==============================] - 149s 16ms/step - loss: 1.1568 - accuracy: 0.6074 - val_loss: 1.5913 - val_accuracy: 0.4433\n",
      "Epoch 69/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.1554 - accuracy: 0.6091 - val_loss: 1.5978 - val_accuracy: 0.4334\n",
      "Epoch 70/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.1428 - accuracy: 0.6082 - val_loss: 1.5915 - val_accuracy: 0.4459\n",
      "Epoch 71/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.1376 - accuracy: 0.6155 - val_loss: 1.6307 - val_accuracy: 0.4255\n",
      "Epoch 72/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.1295 - accuracy: 0.6189 - val_loss: 1.5876 - val_accuracy: 0.4413\n",
      "Epoch 73/100\n",
      "9121/9121 [==============================] - 141s 15ms/step - loss: 1.1232 - accuracy: 0.6200 - val_loss: 1.5734 - val_accuracy: 0.4426\n",
      "Epoch 74/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.1172 - accuracy: 0.6216 - val_loss: 1.5990 - val_accuracy: 0.4383\n",
      "Epoch 75/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.1018 - accuracy: 0.6306 - val_loss: 1.5706 - val_accuracy: 0.4459\n",
      "Epoch 76/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0962 - accuracy: 0.6317 - val_loss: 1.5811 - val_accuracy: 0.4502\n",
      "Epoch 77/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0944 - accuracy: 0.6289 - val_loss: 1.6376 - val_accuracy: 0.4258\n",
      "Epoch 78/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0762 - accuracy: 0.6378 - val_loss: 1.5962 - val_accuracy: 0.4472\n",
      "Epoch 79/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0767 - accuracy: 0.6360 - val_loss: 1.5941 - val_accuracy: 0.4456\n",
      "Epoch 80/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0659 - accuracy: 0.6442 - val_loss: 1.5945 - val_accuracy: 0.4383\n",
      "Epoch 81/100\n",
      "9121/9121 [==============================] - 144s 16ms/step - loss: 1.0585 - accuracy: 0.6415 - val_loss: 1.6220 - val_accuracy: 0.4305\n",
      "Epoch 82/100\n",
      "9121/9121 [==============================] - 141s 16ms/step - loss: 1.0431 - accuracy: 0.6494 - val_loss: 1.6088 - val_accuracy: 0.4367\n",
      "Epoch 83/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0401 - accuracy: 0.6486 - val_loss: 1.5751 - val_accuracy: 0.4459\n",
      "Epoch 84/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 1.0409 - accuracy: 0.6531 - val_loss: 1.5804 - val_accuracy: 0.4446\n",
      "Epoch 85/100\n",
      "9121/9121 [==============================] - 141s 16ms/step - loss: 1.0230 - accuracy: 0.6569 - val_loss: 1.6008 - val_accuracy: 0.4364\n",
      "Epoch 86/100\n",
      "9121/9121 [==============================] - 150s 16ms/step - loss: 1.0190 - accuracy: 0.6686 - val_loss: 1.6016 - val_accuracy: 0.4347\n",
      "Epoch 87/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.0056 - accuracy: 0.6592 - val_loss: 1.5754 - val_accuracy: 0.4476\n",
      "Epoch 88/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 1.0048 - accuracy: 0.6625 - val_loss: 1.6544 - val_accuracy: 0.4199\n",
      "Epoch 89/100\n",
      "9121/9121 [==============================] - 143s 16ms/step - loss: 0.9969 - accuracy: 0.6701 - val_loss: 1.6685 - val_accuracy: 0.4196\n",
      "Epoch 90/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 0.9908 - accuracy: 0.6696 - val_loss: 1.5665 - val_accuracy: 0.4485\n",
      "Epoch 91/100\n",
      "9121/9121 [==============================] - 141s 15ms/step - loss: 0.9783 - accuracy: 0.6715 - val_loss: 1.6156 - val_accuracy: 0.4295\n",
      "Epoch 92/100\n",
      "9121/9121 [==============================] - 141s 16ms/step - loss: 0.9676 - accuracy: 0.6773 - val_loss: 1.5793 - val_accuracy: 0.4482\n",
      "Epoch 93/100\n",
      "9121/9121 [==============================] - 141s 16ms/step - loss: 0.9745 - accuracy: 0.6781 - val_loss: 1.6044 - val_accuracy: 0.4364\n",
      "Epoch 94/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 0.9505 - accuracy: 0.6863 - val_loss: 1.6104 - val_accuracy: 0.4321\n",
      "Epoch 95/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 0.9469 - accuracy: 0.6886 - val_loss: 1.6069 - val_accuracy: 0.4387\n",
      "Epoch 96/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 0.9358 - accuracy: 0.6886 - val_loss: 1.6073 - val_accuracy: 0.4387\n",
      "Epoch 97/100\n",
      "9121/9121 [==============================] - 142s 16ms/step - loss: 0.9300 - accuracy: 0.6896 - val_loss: 1.6036 - val_accuracy: 0.4331\n",
      "Epoch 98/100\n",
      "9121/9121 [==============================] - 141s 16ms/step - loss: 0.9245 - accuracy: 0.6931 - val_loss: 1.5791 - val_accuracy: 0.4443\n",
      "Epoch 99/100\n",
      "9121/9121 [==============================] - 141s 15ms/step - loss: 0.9172 - accuracy: 0.6950 - val_loss: 1.5905 - val_accuracy: 0.4472\n",
      "Epoch 100/100\n",
      "9121/9121 [==============================] - 141s 15ms/step - loss: 0.9141 - accuracy: 0.6996 - val_loss: 1.6185 - val_accuracy: 0.4393\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\n",
    "model_history=model.fit(X_train, y_train, batch_size=16, epochs=100, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model serialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model and weights at /home/bukya/Learning_Purpose/Speech_Emotion_Recognition/saved_models/Emotion_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "# Save model and weights\n",
    "model_name = 'Emotion_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Save model and weights at %s ' % model_path)\n",
    "\n",
    "# Save the model to disk\n",
    "model_json = model.to_json()\n",
    "with open(\"model_json.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 43.93%\n"
     ]
    }
   ],
   "source": [
    "# loading json and model architecture \n",
    "json_file = open('model_json.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# Keras optimiser\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3041/3041 [==============================] - 15s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([11,  3, 11, ...,  0,  5,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = loaded_model.predict(X_test, \n",
    "                         batch_size=16, \n",
    "                         verbose=1)\n",
    "\n",
    "preds=preds.argmax(axis=1)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>male_sad</td>\n",
       "      <td>female_disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>female_surprise</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>female_disgust</td>\n",
       "      <td>female_disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>female_fear</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>male_surprise</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>female_fear</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>female_happy</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>female_neutral</td>\n",
       "      <td>female_neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        actualvalues predictedvalues\n",
       "170         male_sad  female_disgust\n",
       "171  female_surprise      female_sad\n",
       "172       male_angry      male_happy\n",
       "173   female_disgust  female_disgust\n",
       "174       male_angry      male_angry\n",
       "175      female_fear    female_happy\n",
       "176    male_surprise    female_angry\n",
       "177      female_fear      female_sad\n",
       "178     female_happy        male_sad\n",
       "179   female_neutral  female_neutral"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions \n",
    "preds = preds.astype(int).flatten()\n",
    "preds = (lb.inverse_transform((preds)))\n",
    "preds = pd.DataFrame({'predictedvalues': preds})\n",
    "\n",
    "# Actual labels\n",
    "actual=y_test.argmax(axis=1)\n",
    "actual = actual.astype(int).flatten()\n",
    "actual = (lb.inverse_transform((actual)))\n",
    "actual = pd.DataFrame({'actualvalues': actual})\n",
    "\n",
    "# Lets combined both of them into a single dataframe\n",
    "finaldf = actual.join(preds)\n",
    "finaldf[170:180]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>predictedvalues</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>female_angry</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>female_disgust</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>female_fear</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>female_happy</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>female_neutral</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>female_sad</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>female_surprise</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_angry</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_disgust</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_fear</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_happy</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_neutral</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_sad</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>male_surprise</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 actualvalues\n",
       "predictedvalues              \n",
       "female_angry              319\n",
       "female_disgust            502\n",
       "female_fear               229\n",
       "female_happy              425\n",
       "female_neutral            221\n",
       "female_sad                348\n",
       "female_surprise           117\n",
       "male_angry                109\n",
       "male_disgust              124\n",
       "male_fear                  64\n",
       "male_happy                152\n",
       "male_neutral              238\n",
       "male_sad                  144\n",
       "male_surprise              49"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write out the predictions to disk\n",
    "finaldf.to_csv('Predictions.csv', index=False)\n",
    "finaldf.groupby('predictedvalues').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.43932916803683\n"
     ]
    }
   ],
   "source": [
    "# Get the predictions file \n",
    "finaldf = pd.read_csv(\"Predictions.csv\")\n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "\n",
    "# Confusion matrix \n",
    "c = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\n",
    "print(\"Model Accuracy:\", accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "   female_angry       0.54      0.60      0.56       287\n",
      " female_disgust       0.33      0.62      0.43       267\n",
      "    female_fear       0.50      0.41      0.45       282\n",
      "   female_happy       0.40      0.58      0.48       293\n",
      " female_neutral       0.56      0.52      0.54       236\n",
      "     female_sad       0.47      0.60      0.53       277\n",
      "female_surprise       0.88      0.84      0.86       123\n",
      "     male_angry       0.71      0.38      0.49       204\n",
      "   male_disgust       0.30      0.18      0.23       200\n",
      "      male_fear       0.39      0.13      0.19       197\n",
      "     male_happy       0.30      0.22      0.25       202\n",
      "   male_neutral       0.37      0.39      0.38       221\n",
      "       male_sad       0.21      0.14      0.17       209\n",
      "  male_surprise       0.45      0.51      0.48        43\n",
      "\n",
      "       accuracy                           0.44      3041\n",
      "      macro avg       0.46      0.44      0.43      3041\n",
      "   weighted avg       0.45      0.44      0.43      3041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report \n",
    "classes = finaldf.actualvalues.unique()\n",
    "classes.sort()    \n",
    "print(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
